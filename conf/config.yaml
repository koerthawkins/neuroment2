defaults:
  # configure coloured logging
  # NOTE: you need to have the colorlog plugin install (pip install hydra_colorlog)
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# configure hydra job directories
# an example output directory is: './outputs/2022-01-18/my-cool-job'
# override the job directory via `python main.py hydra.job.name=my-cool-job`
hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${hydra.job.name}
  sweep:
    dir: ./multirun/${now:%Y-%m-%d}/${hydra.job.name}
    subdir: ${hydra.job.num}

# general configuration options
class_labels: ["clarinet", "electric_guitar", "female_singer", "flute",
              "piano", "tenor_saxophone", "trumpet", "violin"]
# figure size for plots that will be created
figsize: [8, 4.5]
# level range in db
# predictions are clipped to this range
level_range_in_db: [-60.0, 0.0]

Mixer:
  # absolute path to directory with raw audio files from which features are computed
  raw_data_path: "data/medley-solos-db-small/"
  # path where we write generated features and labels to
  pickle_path: "data/pickle/"
  # the type of data we want to generate. options: "training", "validation", "test"
  data_type: "test"
  # number of times we use each raw data file per mixing epoch
  num_epochs:
    training: 1
    validation: 1
    test: 1
  # the number of instruments will be uniformly sampled from this range
  num_instruments_mix: [1, 4]
  # total number of instruments in the dataset
  num_instruments: 8
  # the length of the observation window in samples times the number of observation windows per file
  # (it's best not to change that)
  num_samples_per_file: 131072
  # the feature computation algorithm with which features will be computed
  # options: CQT, STFT, MEL, CQT+MEL
  feature: "MEL"
  # the algorithm used to compute the envelope/activations
  # options: "RMS" (RMS computed in time domain), "SPECTRUM" (use spectral data to compute envelope)
  envelope_type: "RMS"
  # window function used for STFT
  window: "hann"
  # whether to center STFT frames or not
  # CQT always centers so it's advisable to set that to true and keep it at True
  center: True
  # mel-frequency settings - number of mel frequencies and min/max mel frequencies (only applies if features=="MEL")
  num_mels: 128
  f_min: 27.5
  f_max: 20000
  # CQT settings - number of bins per octave and number of octaves
  num_bins_per_octave: 24
  num_octaves: 8
  Mix:
    # STFT settings
    dft_size: 2048
    hopsize: 1024
    # number of STFT (feature) frames in an observation window
    num_frames: 14
    # sampling rate in Hz
    sr: 44100
    # whether to save WAV data to pickle files after feature computation
    save_wav_data: True

train:
  # the directory with the pickle files containing features and labels (relative to repository root dir)
  dataset_dir: "data/pickle/"
  # the path where we load the most recent checkpoint from (relative to repository root dir)
  model_load_dir: null
  # the path we will save new models to (relative to hydra CWD)
  model_save_dir: "models/"
  # the path where tensorboard logs will be written to (relative to hydra CWD)
  tensorboard_dir: "logs/"
  # at least one of training_steps or training_epochs must be specified as >= 1
  training_steps: -1
  training_epochs: -1
  continue_training: True
  # number of workers used in pytorch dataloader
  num_workers: 0
  # batch size for training and validation data
  batch_size: 32
  # number of batches that are used to average losses
  n_batches_per_average: 100
  # whether to standardize features to 0 mean and 1 var or not
  standardize_features: True
  # if a value > 0 we save model checkpoints at multiples of this interval
  # if -1 we don't save checkpoints
  model_checkpoint_interval: 5000
  gpu_index: 0
  use_batch_norm: True
  dropout_rate: 0.1
  loss_weights:
    bce: 1.0
    mse: 0.0
    bce_per_instrument: 0.0
    frobenius: 0.0
    compressed_spectral: 0.0
  # weight of each instrument in the BinaryCrossentropy-per-instrument loss
  bce_weight_per_instrument:
    clarinet: 1.0
    electric_guitar: 0.5
    female_singer: 0.5
    flute: 2.0
    piano: 0.5
    tenor_saxophone: 2.0
    trumpet: 1.0
    violin: 1.0

inference:
  # directory with audio files to run inference on
  audio_dir: "data/test/"
  # full path to model we want to use for prediction
  model_path: "models/neuroment2_cqt_00125782.model"
  # path where predictions will be saved too
  predictions_dir: "predictions/"
  gpu_index: 0

# parameters for AdamW optimizer
# (see the pytorch documentation for a description of each parameter)
optimizer:
  learning_rate: 0.001
  beta: [0.9, 0.999]
  weight_decay: 0.001

# parameters for ReduceLROnPlateau learning rate scheduler
# (see the pytorch documentation for a description of each parameter)
scheduler:
  mode: "min"
  factor: 0.2
  patience: 8
  threshold: 0.0001
  min_lr: 0.00001

# dataset parsing configuration (for parse_dataset.py)
parsing:
  # the input dataset directory
  # this should be the extracted version of the Medley-Solos DB v1.2 dataset
  input_dataset_dir: "/media/WindowsPartition/Users/maier/Desktop/datasets/MedleyDB/full_dataset_v1p2"
  # the dataset types which we want to parse
  dataset_types: ["training", "validation", "test"]
  # we oversample the underrepresented classes by this factor
  # overrepresented classes will be undersampled s.t. in the end each class has the same number of files
  max_oversampling_factor: 2.0
  # directory where the parsed dataset will be written to
  # we automatically append the max_oversampling_factor to the output_dataset_dir in code
  output_dataset_dir: "data/balanced-dataset/"
  # directory where plots with file distributions will be written to
  output_plot_dir: "plots/"


